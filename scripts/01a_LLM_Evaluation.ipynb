{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f74337-845b-49cc-86de-b12c60231cca",
   "metadata": {},
   "source": [
    "### Import Modules and Set up Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc16b27-d31b-4246-abf8-0fd1997f2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from openai import OpenAI\n",
    "import os, requests, warnings, glob, re, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "my_gemini_api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    "my_gemini_model   = \"gemini-1.5-flash\"\n",
    "\n",
    "OpenAI.api_key    = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_client     = OpenAI(api_key = OpenAI.api_key) \n",
    "headers           = {\"Content-Type\": \"application/json\",\"Authorization\": f\"Bearer {OpenAI.api_key}\"}\n",
    "my_gpt_model      = \"gpt-4o-mini\"\n",
    "\n",
    "def extract_first_integer(text):\n",
    "  \"\"\"Extracts the first integer from a string or returns the integer if already an int.\"\"\"\n",
    "  if isinstance(text, int):\n",
    "    return text\n",
    "  if isinstance(text, str):\n",
    "    match = re.search(r'\\d+', text)\n",
    "    if match:\n",
    "      return int(match.group(0))\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bff16-23e8-4fc3-890a-a8ee538d4ab1",
   "metadata": {},
   "source": [
    "### 1 Define Slices, Paths, and Instruction Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2315d03e-105d-451c-9fd7-bb9b05bc468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [1.0, 0.75, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "\n",
    "speech_transcript_folder = '../data/02_transcripts_corrected/'\n",
    "speech_appendix          = '_cut_corrected.txt'\n",
    "output_folder            = '../data/05_ratings/'\n",
    "\n",
    "# Prompts\n",
    "# Prompt 1: Direct and Minimalist\n",
    "# Prompt 2: Emphasizing Holistic Quality\n",
    "# Prompt 3: Neutral and Instructional\n",
    "# Prompt 4: Framing as an Expert Evaluation\n",
    "# Prompt 5: Avoiding Explicit Criteria While Keeping the Task Clear\n",
    "instruct_prompt_list = [\"Here is a transcript from a public presentation on a science/research topic. Please rate the speech quality on a scale from 1 (worst) to 10 (best). Consider factors such as clarity, engagement, and how easy it is to follow. Return only the single rating number as a plain integer, with no other text or characters. Here is the speech text: \",\n",
    "                        \"You will receive a transcript of a science/research presentation. Rate the overall rhetorical quality on a scale from 1 (worst) to 10 (best), considering clarity, engagement, structure, and delivery. Return only the single rating number as a plain integer, with no other text or characters. Here is the speech text: \",\n",
    "                        \"Given the following transcript of a science/research presentation, assess its overall speech quality. Focus on aspects such as clarity, engagement, and coherence. Provide only a single numerical rating from 1 (worst) to 10 (best), without any additional text. Here is the speech text: \",\n",
    "                        \"Imagine you are an expert in public speaking evaluation. Below is a transcript from a science/research presentation. Please rate the effectiveness of the speech on a scale of 1 (worst) to 10 (best) based on clarity, engagement, and ease of understanding. Return only the single rating number as a plain integer, with no other text or characters. Here is the speech text: \",\n",
    "                        \"Please evaluate the following transcript of a public science/research presentation. Assign a quality rating from 1 (worst) to 10 (best) based on your assessment. Return only a single rating number as a plain integer, with no other text or characters. Here is the speech text: \"]\n",
    "num_instruction_prompts = len(instruct_prompt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ffaa0-4a1c-4dcc-a138-103a4b4aca19",
   "metadata": {},
   "source": [
    "### 2 Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce219969-4df0-421c-9f8f-5f02b04d7a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SUB001_SPEECH001', 'SUB002_SPEECH001', 'SUB003_SPEECH001']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df           = pd.read_csv('../data/list_of_subjects.csv')\n",
    "speech_names = list(df.name.values)\n",
    "num_speeches = len(speech_names)\n",
    "print(num_speeches)\n",
    "speech_names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469abe18-f1f3-4ef9-a469-7dfd9f98c226",
   "metadata": {},
   "source": [
    "### 3 Concept Prompt and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c4f3ec-e1f9-43fc-9a27-2d7a146a12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Slice Length:\n",
      "1.0\n",
      "Current Slice Length:\n",
      "0.75\n",
      "Current Slice Length:\n",
      "0.5\n",
      "Current Slice Length:\n",
      "0.4\n",
      "Current Slice Length:\n",
      "0.3\n",
      "Current Slice Length:\n",
      "0.2\n",
      "Current Slice Length:\n",
      "0.1\n",
      "Current Slice Length:\n",
      "0.05\n",
      "Current Slice Length:\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "# loop over slices\n",
    "for curr_slice in slices: \n",
    "    print('Current Slice Length:')\n",
    "    print(curr_slice)\n",
    "\n",
    "    # loop over different prompting strategies\n",
    "    for curr_instruct_prompt_index in range(num_instruction_prompts):\n",
    "        #print('Current Instruction Prompt:')\n",
    "        #print(curr_instruct_prompt_index)\n",
    "        curr_instruct_prompt_text = instruct_prompt_list[curr_instruct_prompt_index]\n",
    "    \n",
    "        # loop over the individual speeches\n",
    "        gemini_names, gemini_ratings, openai_names, openai_ratings = [], [], [], []\n",
    "        for curr_speech_name in speech_names:#[:1]:\n",
    "            #print(curr_speech_name)\n",
    "    \n",
    "            # read in the speech text and limit to the part defined by the slice length\n",
    "            input_text_file = speech_transcript_folder + curr_speech_name + speech_appendix\n",
    "            with open(input_text_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                 content = file.read()\n",
    "            index = int(len(content)* curr_slice)  #this is the part where the text corresponding to the subslice is limited/selected\n",
    "            part_of_speech = content[0:index]    \n",
    "\n",
    "            # create the full prompt to be submitted and submit to LLM\n",
    "            full_prompt = curr_instruct_prompt_text + part_of_speech\n",
    "                \n",
    "            # Google Gemini     \n",
    "            client = genai.Client(api_key=my_gemini_api_key)\n",
    "            response = client.models.generate_content(model=my_gemini_model, contents=full_prompt)\n",
    "            llm_response_gemini =int(response.text[0])  \n",
    "            gemini_names.append( str(curr_speech_name)) \n",
    "            gemini_ratings.append(int(extract_first_integer(llm_response_gemini)))\n",
    "\n",
    "            # OpenAI GPT4o mini\n",
    "            payload = {\"model\": my_gpt_model, \"messages\": [{ \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": full_prompt  },]}],}\n",
    "            response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "            llm_response_openai  = response.json()['choices'][0]['message']['content']\n",
    "            openai_names.append( str(curr_speech_name)) \n",
    "            openai_ratings.append(int(extract_first_integer(llm_response_openai)))\n",
    "        \n",
    "    \n",
    "        # convert results to df and save\n",
    "        df = pd.DataFrame({\"name\": gemini_names, \"rating\": gemini_ratings})        \n",
    "        out_name = output_folder + \"corrected_transcripts_gemini1.5flash_slice#\" + str(curr_slice) + \"_prompt#\"  + str(curr_instruct_prompt_index + 1) +  \"_rating.csv\"\n",
    "        df.to_csv(out_name, index=False)\n",
    "\n",
    "        df = pd.DataFrame({\"name\": openai_names, \"rating\": openai_ratings})        \n",
    "        out_name = output_folder + \"corrected_transcripts_gpt4omini_slice#\" + str(curr_slice) + \"_prompt#\"  + str(curr_instruct_prompt_index + 1) +  \"_rating.csv\"\n",
    "        df.to_csv(out_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e6037-1c4a-4354-b94c-883b4ffba99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4eea4e-91fb-4285-9f60-30cb30715495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
